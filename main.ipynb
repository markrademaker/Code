{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install moviepy\n",
    "#!pip install transformers torchaudio\n",
    "#!pip install spleeter\n",
    "#!pip install torchaudio\n",
    "import media_embedder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video loaded successfully: /Users/markrademaker/Downloads/Work/Scriptie/Data/BBC_Planet_Earth_Dataset/bbc_01.mp4\n",
      "MoviePy - Writing audio in /Users/markrademaker/Downloads/Work/Scriptie/Data/BBC_Planet_Earth_Dataset/bbc_01_audio.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Audio extracted: /Users/markrademaker/Downloads/Work/Scriptie/Data/BBC_Planet_Earth_Dataset/bbc_01_audio.mp3\n",
      "Moviepy - Building video /Users/markrademaker/Downloads/Work/Scriptie/Data/BBC_Planet_Earth_Dataset/bbc_01_no_audio.mp4.\n",
      "Moviepy - Writing video /Users/markrademaker/Downloads/Work/Scriptie/Data/BBC_Planet_Earth_Dataset/bbc_01_no_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/markrademaker/Downloads/Work/Scriptie/Data/BBC_Planet_Earth_Dataset/bbc_01_no_audio.mp4\n",
      "Video without audio created: /Users/markrademaker/Downloads/Work/Scriptie/Data/BBC_Planet_Earth_Dataset/bbc_01_no_audio.mp4\n"
     ]
    }
   ],
   "source": [
    "from video_loader import VideoLoader\n",
    "#video_loader = VideoLoader(\"/Users/markrademaker/Downloads/Work/Scriptie/Data/Survivor_full.mov\")\n",
    "#video_loader.load_video()\n",
    "\n",
    "#video_loader.extract_subpart(0,100,\"/Users/markrademaker/Downloads/Work/Scriptie/Data/Survivor_0-100.mov\")\n",
    "sub_video_loader=VideoLoader(\"/Users/markrademaker/Downloads/Work/Scriptie/Data/BBC_Planet_Earth_Dataset/bbc_01.mp4\")\n",
    "sub_video_loader.load_video()\n",
    "sub_video_loader.split_audio_video()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'separate_audio_sources' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/markrademaker/Downloads/Work/Scriptie/Code/main.ipynb Cell 4\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/markrademaker/Downloads/Work/Scriptie/Code/main.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m separate_audio_sources(\u001b[39m'\u001b[39m\u001b[39m/Users/markrademaker/Downloads/Work/Scriptie/Data/Survivor_full_audio.mp3\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m/Users/markrademaker/Downloads/Work/Scriptie/Data/Survivor_full_audio_source.mp3\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'separate_audio_sources' is not defined"
     ]
    }
   ],
   "source": [
    "separate_audio_sources('/Users/markrademaker/Downloads/Work/Scriptie/Data/Survivor_full_audio.mp3', '/Users/markrademaker/Downloads/Work/Scriptie/Data/Survivor_full_audio_source.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: av in /usr/local/anaconda3/lib/python3.9/site-packages (11.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install av"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN and Wav2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video's frame rate is: 25.0 FPS\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import media_embedder\n",
    "import cv2\n",
    "import media_embedder\n",
    "from media_embedder import MediaEmbedder\n",
    "video_path = \"/Users/markrademaker/Downloads/Work/Scriptie/Data/Survivor_0-100_no_audio.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "print(f\"The video's frame rate is: {fps} FPS\")\n",
    "\n",
    "cap.release()\n",
    "\n",
    "media_embed = MediaEmbedder()\n",
    "chunk_duration=1 #seconds for synchronization\n",
    "audio_embeddings = media_embed.embed_audio(\n",
    "    \"/Users/markrademaker/Downloads/Work/Scriptie/Data/Survivor_0-100_audio.mp3\", chunk_duration)\n",
    "#video_embeddings = media_embed.embed_video(\n",
    "#    \"/Users/markrademaker/Downloads/Work/Scriptie/Data/Survivor_0-100_no_audio.mp4\", chunk_duration, fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import biGRU\n",
    "audio_file_paths=[\"/Users/markrademaker/Downloads/Work/Scriptie/Data/BBC_Planet_Earth_Dataset/bbc_01_audio.mp3\"]\n",
    "embedder = MediaEmbedder()\n",
    "chunk_duration=1\n",
    "# For simplicity, this example will handle one file at a time\n",
    "embeddings_list = []\n",
    "labels_list = []  # Assuming you have a way to get labels for each audio file\n",
    "for audio_path in audio_file_paths:\n",
    "    embeddings,labels = embedder.embed_audio(audio_path, chunk_duration)\n",
    "    embeddings_list.append(embeddings)\n",
    "    # Append corresponding label here\n",
    "    labels_list.append(labels)  # You need to define how to get this\n",
    "\n",
    "# Convert embeddings and labels into a dataset\n",
    "embeddings_tensor = torch.stack(embeddings_list)\n",
    "labels_tensor = torch.tensor(labels_list)\n",
    "\n",
    "embedding_dataset = EmbeddingsDataset(embeddings_tensor, labels_tensor)\n",
    "train_loader = DataLoader(embedding_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Continue with the instantiation of the BiGRU model and training as before\n",
    "audio_model = AudioBiGRU(input_size=embeddings_tensor.size(-1), hidden_size=128, num_layers=2, num_classes=2)\n",
    "trainer = AudioModelTrainer(audio_model, train_loader, learning_rate=0.001, num_epochs=10)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YAMnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yamnet = YAMnetClassifier()\n",
    "audio_data = your_audio_data_here  # This should be a waveform array\n",
    "yamnet_predictions = yamnet.predict(audio_data)\n",
    "yamnet_class_input = yamnet.process(audio_data)\n",
    "yamnet_embedding = yamnet.get_embedding(audio_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_classifier = VGG16Classifier()\n",
    "img_path = 'path_to_your_image.jpg'\n",
    "vgg_features = vgg16_classifier.get_features_before_last_layer(img_path)\n",
    "vgg_predictions = vgg16_classifier.predict(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming labels and embeddings are defined\n",
    "scene_labels = [0,1]  # Replace with your scene labels\n",
    "# Create a combined dataset and data loader\n",
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, audio_embeddings, video_embeddings, labels):\n",
    "        self.audio_embeddings = audio_embeddings\n",
    "        self.video_embeddings = video_embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.audio_embeddings[idx], self.video_embeddings[idx], self.labels[idx]\n",
    "\n",
    "combined_dataset = CombinedDataset(audio_embeddings, video_embeddings, scene_labels)\n",
    "combined_loader = DataLoader(combined_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create an IntegratedModelTrainer instance\n",
    "trainer = IntegratedModelTrainer(audio_model, video_model, mlp_model, combined_loader)\n",
    "\n",
    "# Train the integrated model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later, for fine-tuning only the MLP\n",
    "trainer.toggle_biGRU_trainability(trainable=False)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
